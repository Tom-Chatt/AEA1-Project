{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Can the GBP/USD exchange rate be effectively forecasted using an ARIMA model?\"\n",
    "AUTHOR = \"Tom Chatt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python assignment\n",
    "\n",
    "The python assignment you can do either on your own or with one other student (i.e. max group size is 2 students).\n",
    "\n",
    "The first cell of your notebook, should contain a table with the names and SNRs and UNRs of the group members, like so\n",
    "\n",
    "|Name|SNR|UNR|\n",
    "|----|---|----|\n",
    "|Tom Chatt|*******|*******|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "See [the webpage](https://janboone.github.io/applied-economics/#org065a005) for details of what we expect to see in this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b350c3082e112a5b2f1f6db0481fa5bd",
     "grade": true,
     "grade_id": "cell-7a61a5e9e6991ff4",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### Can the GBP/USD exchange rate be effectively forecasted using an ARIMA model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ab93f4d26eb3e0bcf88927ee71df6b63",
     "grade": true,
     "grade_id": "cell-3d71712c92143820",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The United States (US) is the single largest country that the UK trades with (Office for National Statistics, 2018). The UK's trade relationship with the USA is very interesting, it is the economy where the UK has the largest trade surplus (with £99.569 billion in exports and imports of £66.313 billion). With Brexit on the horizon, a trade deal with the US is very likely and with it, increased trade. However, an appreciating currency would dampen the increase in demand associated with these trade agreements. \n",
    "\n",
    "<br>\n",
    "\n",
    "The exchange rate analysed in this document is GBP/USD, which can be interpreted as the number of US Dollars ($) one Pound Sterling (£) buys. Export demand is affected by the price of the exports, a depreciation in the Pound would mean that exports are comparatively cheaper than they were before. As such, it is likely that US import demand would increase for goods from the UK (Parker & Pickard, 2020).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84c7d0d85d8fb5e0073f3053f9bedf5a",
     "grade": true,
     "grade_id": "cell-8cbf4938fbebbb6e",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The method used for this forecast is ARIMA, which stands for Auto Regressive Integrated Moving Average. This methodology was first used by Box & Jenkins (1976) who devised the ARIMA methodology. This assignment also follows loosely the Box Jenkins methodology, which has three steps: <br>\n",
    "1. Identification - using the data to select the appropriate model order.\n",
    "2. Estimation - using the data to train the model and find parameter estimates\n",
    "3. Diagnostics - looking at how to improve the model. \n",
    "\n",
    "Three types of forecasts will be made:\n",
    "1. One step ahead forecast - where the ARIMA model uses all data up to and including period i and forecasts only for period i+1.\n",
    "2. Dynamic forecast - the ARIMA model forecasts the full period i+k for a given number of periods k, using data up to and including period i. \n",
    "3. Out of sample forecasts - the ARIMA model forecasts for a given period in the future, for which there is no data. \n",
    "\n",
    "### Stationarity\n",
    "There are many steps within the Identification stage, assessing the Stationarity of the time series is very important. Stationarity means that the distribution of the data does not change with time. It must fulfil three criteria:\n",
    "  1. The series has zero trend (e.g it is not growing over time)\n",
    "  2. The varience is constant\n",
    "  3. The Autocorrelation is constant (how each value in the time series is related to its neighbours stays the same)\n",
    "\n",
    "Hamilton (1994) defines a process as stationary in the following way, if neither the mean $ \\mu_t $ or the autocovariances $ \\gamma _{jt} $ depend on the date t, then the process $ Y_t $ is said to be weakly stationary. This is formally defined in the following way: $$ E(Y_t) = \\mu \\quad \\quad for\\; all \\;t $$\n",
    "$$ E(Y_t - \\mu)(Y_{t-j} - \\mu) = \\gamma_j \\quad \\quad for \\;all \\;t \\;and \\;any\\; j $$\n",
    "\n",
    "\n",
    "In order to test for weak stationarity in the model the Augmented Dickie Fuller test (ADF) is used. This tests whether there is a Unit Root within the time-series data. \n",
    "\n",
    "\n",
    "##### Augmented Dickie Fuller Test\n",
    "\n",
    "\n",
    "Phillips, P. (1987)<br>\n",
    "$$ \\Delta y = \\mu + \\gamma^* y_{t-1} + \\sum_{j=1}^{p-1} \\theta_j \\Delta y_{t-j} + \\epsilon_t $$ <br>\n",
    " Where p is the auto regressive order.  \n",
    "The augmented dickie fuller test, tests to see whether $ \\gamma^* = 0 $ , the model is non-stationary or has unit root present if $ \\gamma^* = 0 $, <br><br>\n",
    "As such the null hypothesis is: $ H_0 : \\gamma^*=0 $ <br><br>\n",
    "\n",
    "### Selecting the best model\n",
    "#### Akaike Information Criterion (AIC)\n",
    "AIC was originally formulated by Hirotugu Akaike in the paper Akaike, H. (1974), as an estimator of out of sample prediction error and thus quality, of a given estimator. The AIC then generates a score with the formula: $$ AIC = 2k-2ln(\\hat{L}) $$\n",
    "The model with the lowest AIC score is preferred. The absolute score is unimportant and instead the relative score should be compared when deciding which model order is chosen. (Burnham and Anderson, 2002)\n",
    "\n",
    "### Limitations\n",
    "ARIMA is not without fault, however with the papers Pai and Lin (2005) and Zhang (2003) both being critical of ARIMA. They state a range of limitations of the model, including the fact that the model must have a linear function form, which does not approximate a lot of real-world time series data accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Cox\n",
    "Box Cox  aims to transform the data so that it follows a normal distribution. Box & Cox (1964) <br>\n",
    "The Box Cox equation is as follows: $$ x(\\lambda) = \\frac{(x^\\lambda - 1)}{\\lambda} \\quad \\quad for \\; \\lambda \\neq \\; 0 $$\n",
    "$$ x(\\lambda) = ln(x) \\quad \\quad for \\; \\lambda \\; = \\; 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "938eb25dc709a2252d7013dc1cd8176a",
     "grade": true,
     "grade_id": "cell-1b3df4385bba508d",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The quality of the ARIMA predictions vary by forecast type, the one-step-ahead forecasts yield estimates very close to the ex-post values for the GBP/USD exchange rate. Conversely, the dynamic forecast captures the loose overall trend, but none of the large fluctuations seen in the dataset. It is impossible to state the accuracy of the out of sample forecast since it is ex-ante. However, from historical data we can assume that the GBP/USD exchange rate is highly unlikely to remain at a constant value of 1.307 (that being £1=$1.307)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2e95c3f2ef005a63968a0ade420f5af7",
     "grade": true,
     "grade_id": "cell-d84f19967b2baa1d",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "##### For the accurate use of this model, we must assume the following is true (Box & Jenkins, 1976):\n",
    "1. No suspected predictor variables are present in the model.\n",
    "2. Level shifts are not present in the analysis.\n",
    "3. Deterministic linear time trends are not present.\n",
    "4. Seasonal dummies are not used.\n",
    "5. One-time anomalies are not present in the data.\n",
    "6. Parameters of the model remain constant\n",
    "7. A homoscedastic error process is present over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this assignment is imported using the Quandl API. The Bank of England published this time series, which is daily data on the GBP/USD exchange rate. More information on the data can be found below:<br>\n",
    "https://www.bankofengland.co.uk/boeapps/database/index.asp?Travel=NIxAZxRSx&TD=14&TM=Feb&TY=2019&into=GBP&CurrMonth=1&startDD=15&startMM=1&startYYYY=2018&From=Rates&C=C8P&G0Xtop.x=1&G0Xtop.y=1<br><br>\n",
    "The code for the exact exchange rate is XUDLUSS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code\n",
    "\n",
    "Give the python code in code cells and use markdown cells to explain why you code things in this way and what the outcomes are of the code cells.\n",
    "\n",
    "Create as many python and markdown cells as you need to explain things well.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3dd4c02198f7b81f943268cb3d0a12b2",
     "grade": true,
     "grade_id": "cell-a2231ee7e6d4e686",
     "locked": false,
     "points": 7,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Installing some of the nescessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import quandl\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a parameter to 'today' allows the code to forecast using the most up to date data available from Quandl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime('today')\n",
    "today = today.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the Quandl API to import the data into the dataframe. XUDLUSS is the Bank of Englands code for data on the GBP-USD exchange rate. The code also normalises the index values and sets the dataframe values to be numeric. 'convert_date=True' results in Pandas changing the index to be in DateTime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "(Status 400) (Quandl Error QEAx01) We could not recognize your API key: *******. Please check your API key and try again. You can find your API key under your account settings.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4bf7da9bf6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*******\"\u001b[0m \u001b[0;31m# replace the asterisks with your QUANDL key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BOE/XUDLUSS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2000-01-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2020-01-24\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set end_date=today for dataset to include data until todays date,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#for validity of results end_date=\"2020-01-24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ensures the exchange rate values are numeric and not strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/get.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_column_not_found\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/model/dataset.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mupdated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mupdated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle_not_found_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/operations/list.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(cls, **options)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                        **options)\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/quandl/connection.py\u001b[0m in \u001b[0;36mhandle_api_error\u001b[0;34m(cls, resp)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuandlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m: (Status 400) (Quandl Error QEAx01) We could not recognize your API key: *******. Please check your API key and try again. You can find your API key under your account settings."
     ]
    }
   ],
   "source": [
    "quandl.ApiConfig.api_key = \"*****\" # replace the asterisks with your QUANDL key\n",
    "df = quandl.get(\"BOE/XUDLUSS\", start_date=\"2000-01-01\", end_date=\"2020-01-24\", convert_date=True) #set end_date=today for dataset to include data until todays date,\n",
    "#for validity of results end_date=\"2020-01-24\"\n",
    "df.index=df.index.normalize()\n",
    "df.Value = pd.to_numeric(df.Value) # ensures the exchange rate values are numeric and not strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below prints the first and last dates in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = df.index[0].date()\n",
    "end = df.index[-1].date()\n",
    "'The first date in the series is ' + str(start) + ' and the last date in the timeseries is ' + str(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving missing data issues\n",
    "When running the ARIMA forecast I ran into an issue with the indexes. The UK FOREX markets are closed on Bank Holidays and as such no data is included in the dataset. This means that the ARIMA process cannot infer the frequency, when making out of sample forecasts. In order to overcome this issue I tried many things, but settled on the following methodology:\n",
    "1. Update the Index to include all days from the first day of the timeseries to the last day.\n",
    "2. Use the fillna 'ffill' method to fill in the missing days (at this stage it fills in weekends and bank holidays). This method fills in missing data by using the previous value. This makes sense, since for days when the market is closed, we assume the price takes the closing price from the last open day.  \n",
    "3. The final step is to to update the index to only include 5 days a week (Monday to Friday) and remove the weekend values.\n",
    "<br><br>\n",
    "\n",
    "This has the result of filling bank holidays with the exchange rate from the day before, but not including weekend values since the markets are not open. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range(start, end)\n",
    "df = df.reindex(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', inplace=True)\n",
    "df = df[df.index.dayofweek < 5]\n",
    "df.head(7) # head(7) means that we can make sure that weekends are not included in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dataset to only 2010-2020 should improve the forecast, since the recession of 2007-2008 acts as a large shock and could bias the forecasts, compared to the true ex-post values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.index >= '2010-01-01']\n",
    "col_names = ['GBPUSD']\n",
    "df.columns = col_names\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing df.info shows that the pandas dataframe has 2626 entries, these are of a float64 file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.describe shows summary statistics of the dataset. The mean is 1.47 (3 sig fig) which equates to £1 buying \\\\$1.47 USD. There is a large range in the dataset, from a min of 1.21 to a maximum of 1.71."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize =(8,5), dpi=100)\n",
    "df.plot(ax=ax)\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.xlabel('Year')\n",
    "plt.title('GBP/USD exchange rate for the years 2010-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, BoxSelectTool, ColumnDataSource, Band\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bokeh_instructions.png \"Making Bokeh Interactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to activate the interactive scroll zoom functionality of the Bokeh graphs, press the button highlighted above in the red square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool(tooltips=[('Date', '@index{%F}'),\n",
    "                           ('Exchange Rate', '@GBPUSD')], formatters={'index': 'datetime'})\n",
    "source=ColumnDataSource(df)\n",
    "plot = figure(x_axis_type='datetime', x_axis_label = 'Year', y_axis_label = 'Spot exchange rate, US \\$ into Sterling £', tools=[hover, 'pan', 'wheel_zoom'], plot_width=1200)\n",
    "plot.line(x='index', y='GBPUSD', line_width=1.5, selection_color='blue', legend='GBP-USD', source=source)\n",
    "plot.circle(df.index, df.GBPUSD, fill_color='white', size=.3)\n",
    "plot.legend.location = 'top_left'\n",
    "plot.legend.background_fill_color = 'lightgrey'\n",
    "\n",
    "output_file = ('GBPUSD.html')\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset wil be split into a test and a train set. This is done so that we can test the quality of the model fit. This assignment will use the standard split of assigning 3/4 of the data to the training set and 1/4 of the data to the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = round(len(df)*(3/4))\n",
    "df_train = df[:split_point]\n",
    "len_train = len(df_train)\n",
    "df_test = df[split_point:]\n",
    "len_test = len(df_test)\n",
    "print('The first day of the testing dataset is '+str(df_test.index[0].date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure as fig\n",
    "fig, ax = plt.subplots(figsize =(8,5), dpi=100)\n",
    "df_train.plot(ax=ax)\n",
    "df_test.plot(ax=ax)\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.xlabel('Year')\n",
    "plt.title('GBP/USD exchange rate for the years 2010-2020, showing training and testing sets')\n",
    "ax.legend([\"Training Set\", \"Testing Set\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the data is staionary the ADF test is used below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the P-value is 0.614 (3 dp) we fail to reject the null hypothesis that the dataset is stationary. Therefore it is necessary to difference the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(8,5), dpi=100)\n",
    "df_diff.plot(ax=ax)\n",
    "plt.ylabel('$\\Delta$ Spot exchange rate, US \\$ into Sterling')\n",
    "plt.xlabel('Year')\n",
    "plt.title('First difference of the GBP/USD exchange rate')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph looks much more promising, the data looks to be stationary. Again, below we test for stationarity using the ADF test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_diff = adfuller(df_diff.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result_diff[0]))\n",
    "print('p-value: {}'.format(result_diff[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result_diff[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P value is 0.0 which is < 0.05. Therefore the null hypothesis (that the time series is non-stationary), is rejected at a 1% significance level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next step is to find the correct model order. ACF and PACF are used for this.\n",
    "\n",
    "ACF is the Autocorrelation Function. <br>\n",
    "- lag 1 autocorrelation is given by $$\\rightarrow corr(y_t,y_{t-1})$$\n",
    "- lag n autocorrelation is given by $$\\rightarrow corr(y_t,y_{t-n})$$<br>\n",
    "\n",
    "PACF is the Partial Autocorrelation Function and it shows the correlation between a time series and the lagged version of itself after subtracting the effect of correlation at smaller lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_2 = df_diff.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "\n",
    "# make ACF plot\n",
    "plot_acf(df, lags=50, zero=False, ax=ax1) # the value at 0 always =1 so setting zero=False means that this is not plotted\n",
    "\n",
    "#make PACF plot\n",
    "plot_pacf(df, lags=50, zero=False, ax=ax2)\n",
    "\n",
    "# make ACF plot for first order differenced data\n",
    "plot_acf(df_diff, lags=50, zero=False, ax=ax3, title='First Difference Autocorrelation') \n",
    "\n",
    "#make PACF plot for first order differenced data\n",
    "plot_pacf(df_diff, lags=50, zero=False,  ax=ax4, title='First Difference Partial Autocorrelation'); \n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF function for the non-differenced data set starts high and tails off very slowly, which suggesting that the dataset is non stationary. This is not the case in the differenced dataset and therefore, the first difference will be used in the final ARIMA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to find the correct order of the ARIMA model\n",
    "### AIC - Akaike information criterion\n",
    "A model which makes better predictions of the data is given a lower AIC score.\n",
    "\n",
    "### BIC - Bayesian information criterion\n",
    "Is a similar criterion to AIC but prefers simpler models with less parameters.<br><br>\n",
    "AIC is better at choosing predictive models.\n",
    "\n",
    "### Which is best? \n",
    "\n",
    "In the case of predicting exchange rates, the most desirable model is the one with the highest predictive power. As such, we pick the model with the lowest AIC score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "order_aic_bic=[]\n",
    "#loop over AR order\n",
    "for p in range(5):\n",
    "     for d in range(5):   \n",
    "    #loop over MA order:\n",
    "        for q in range(5):\n",
    "            #fit the model\n",
    "            model = SARIMAX(df, order=(p,d,q), trend='c')\n",
    "            results=model.fit()\n",
    "            # print the model order and the AIC/BIC scores\n",
    "            order_aic_bic.append((p, d, q, results.aic, results.bic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be preferable to run the above code, however with the for-loops it creates 125 different options of ARIMA models to run. Therefore, it makes more sense to set parameter d=1 (the ADF test shows that after one difference the data is stationary) and therefore only have two for-loops reduces the number of ARIMA models to 16. A far easier realistic computational task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# in order to mute the errors relating to inferred frequency \n",
    "order_aic_bic=[]\n",
    "#loop over AR order\n",
    "for p in range(4):\n",
    "    #loop over MA order:\n",
    "    for q in range(4):    \n",
    "        #fit the model\n",
    "        model = SARIMAX(df, order=(p,1,q), trend='c')\n",
    "        results=model.fit()\n",
    "        # print the model order and the AIC/BIC scores\n",
    "        order_aic_bic.append((p, q, results.aic, results.bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = pd.DataFrame(order_aic_bic, columns = ['p', 'q', 'aic', 'bic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_aic = order_df.sort_values('aic')\n",
    "order_aic_sorted = order_aic.reindex(order_aic.aic.abs().sort_values(ascending=False).index) # sorts the data so that the result with the lowest value comes first. \n",
    "order_aic_sorted_best = order_aic_sorted.iloc[0] # sets the model order with the lowest value to the variable order_airc_sorted_best\n",
    "print(order_aic_sorted.head(3))\n",
    "print(order_aic_sorted_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest AIC score is preferred. The absolute score is not important, it is the relative score which matters. Since all of these scores are negative, reversing the sort order allows the model with the lowest AIC score to appear first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model order=(012) has the lowest AIC score and thus has the best predictive power of the ARIMA models run. Therefore an ARIMA(0,1,2) model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lev =int(order_aic_sorted_best.values[0])\n",
    "d_lev =1\n",
    "q_lev =int(order_aic_sorted_best.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code sets parameter values for P and Q, based off of the sorting function to find the lowest AIC values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model using order=(0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model=SARIMAX(df, order=(p_lev,d_lev,q_lev), trend='c')\n",
    "results = model.fit()\n",
    "#assign residuals to variable\n",
    "resid = results.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid.plot(kind='kde', title='Density plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean absolute errors of the residuals\n",
    "mae = np.mean(np.abs(resid))\n",
    "results.plot_diagnostics(figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the plots\n",
    "#### Standardized residuals\n",
    "There are no clear structures in these residuals\n",
    "#### Histogram plus estimated density\n",
    "The smoothed histogram (green line) and normal distribution (orange line) are quite similar, which suggests the models are a good fit. The smoothed histogram line has a higher peak than the normal distribution but they follow a similar shape.\n",
    "#### Normal Q-Q\n",
    "Most of the residuals lie on the red line with only a small number at either end lying off of the red line. This is again positive, regarding a good model fit. The plot does show a small sigmoid-curve relationship, suggesting some Heteroskedasticity could be present. \n",
    "#### Correlogram\n",
    "There is little correlation within the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the test statistics\n",
    "Ljung-Box - tests whether the residuals are uncorrelated with each other: $$H_0:E(\\epsilon_t | \\epsilon_{t-z}) = 0   \\quad \\quad z \\in (0,t)$$    \n",
    "Therefore, at a 5% significance level, the null hypothesis is rejected since 0.19 > 0.05 <br><br>\n",
    "Jarque-Bera - tests whether the data are distributed normally: $$H_0: \\epsilon \\sim N(\\mu, \\sigma^2)$$  <br><br>\n",
    "The null hypothesis (that the data is normally distributed) is rejected and the data does not have the characteristics of Normality (where the data points are distributed normally)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Testing\n",
    "\n",
    "## Residuals\n",
    "The ARIMA(0,1,2) model fails the Jarque-Bera test. This suggests that the data is not normally distributed and therefore the next step is to try and run the regression with transformed Data.\n",
    "\n",
    "### Log\n",
    "First it seems appropriate to take the log of the series and repeat the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = np.log(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the same analysis as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(8,5), dpi=100)\n",
    "df_log.plot(ax=ax)\n",
    "plt.ylabel('Log spot exchange rate, US \\$ into Sterling £')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Log GBP/USD exchange rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df_log.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is non-stationary since we fail to reject the ADF null-hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_diff = df_log.diff().dropna() # taking the first difference of the transformed series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(15, 7))\n",
    "\n",
    "# make ACF plot\n",
    "plot_acf(df_log, lags=50, zero=False, ax=ax1) # the value at 0 always =1 so setting zero=False means that this is not plotted\n",
    "\n",
    "#make PACF plot\n",
    "plot_pacf(df_log, lags=50, zero=False, ax=ax2)\n",
    "\n",
    "# make ACF plot for first order differenced data\n",
    "plot_acf(df_log_diff, lags=50, zero=False, ax=ax3, title='First Difference Autocorrelation') \n",
    "\n",
    "#make PACF plot for first order differenced data\n",
    "plot_pacf(df_log_diff, lags=50, zero=False,  ax=ax4, title='First Difference Partial Autocorrelation'); \n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df_log_diff.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now stationary after taking the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# in order to mute the errors relating to inferred frequency \n",
    "order_aic_bic_log=[]\n",
    "#loop over AR order\n",
    "for p in range(4):\n",
    "    #loop over MA order:\n",
    "    for q in range(4):\n",
    "        #fit the model\n",
    "        model_log = SARIMAX(df_log, order=(p,1,q), trend='c')\n",
    "        results_log=model_log.fit()\n",
    "        # print the model order and the AIC/BIC scores\n",
    "        order_aic_bic_log.append((p, q, results_log.aic, results_log.bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_log = pd.DataFrame(order_aic_bic_log, columns = ['p', 'q', 'aic', 'bic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_aic_log = order_df_log.sort_values('aic')\n",
    "order_aic_sorted_log = order_aic_log.reindex(order_aic_log.aic.abs().sort_values(ascending=False).index)\n",
    "order_aic_sorted_best_log = order_aic_sorted_log.iloc[0]\n",
    "print(order_aic_sorted_log.head(3))\n",
    "print(order_aic_sorted_best_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model 012 has the lowest AIC score and thus has the best predictive power of the ARIMA models run. Therefore an ARIMA(0,1,2) model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_log =int(order_aic_sorted_best_log.values[0])\n",
    "d_log =1\n",
    "q_log =int(order_aic_sorted_best_log.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code sets parameter values for P and Q based off of the sorting function to find the lowest AIC values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model_log=SARIMAX(df_log, order=(p_log,d_log,q_log), trend='c')\n",
    "#fit the model\n",
    "results_log = model_log.fit()\n",
    "#assign residuals to variable\n",
    "resid_log = results_log.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_log.plot(kind='kde', title='Density Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean absolute errors of the residuals\n",
    "mae_log = np.mean(np.abs(resid_log))\n",
    "results_log.plot_diagnostics(figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_log.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The results are not much different from the level case. As such, this will not be accepted as a superior alternative for the forecasting. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Box Cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data still does not follow a normal distribution, the next thing to try is using a Box Cox transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_boxcox = np.asarray(df.GBPUSD.values)\n",
    "np_boxcox = np_boxcox.squeeze() # to remove single dimensional entries from the shape array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_cox = boxcox(np_boxcox) #performs the Box Cox transformation\n",
    "np_box_cox = np.asarray(box_cox) # converts the Box Cox input into a numpy array\n",
    "np_box_cox = np.hstack(np_box_cox) # horizontally stacks the array  in sequence \n",
    "np_box_cox = np_box_cox[:-1] #reverse the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxcox = pd.DataFrame(data=np_box_cox, index=df.index) #converts the numpy array into a data frame using the index of the original datadrame to this dataframe. \n",
    "col_names = ['GBPUSD']\n",
    "df_boxcox.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxcox_diff = df_boxcox.diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the same analysis as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(8,5), dpi=100)\n",
    "df_boxcox.plot(ax=ax)\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df_boxcox.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(15, 7))\n",
    "\n",
    "# make ACF plot\n",
    "plot_acf(df_boxcox, lags=50, zero=False, ax=ax1) # the value at 0 always =1 so setting zero=False means that this is not plotted\n",
    "\n",
    "#make PACF plot\n",
    "plot_pacf(df_boxcox, lags=50, zero=False, ax=ax2)\n",
    "\n",
    "# make ACF plot for first order differenced data\n",
    "plot_acf(df_boxcox_diff, lags=50, zero=False, ax=ax3, title='First Difference Autocorrelation') \n",
    "\n",
    "#make PACF plot for first order differenced data\n",
    "plot_pacf(df_boxcox_diff, lags=50, zero=False,  ax=ax4, title='First Difference Partial Autocorrelation'); \n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(df_boxcox_diff.GBPUSD)\n",
    "print('ADF Statistic: {}'.format(result[0]))\n",
    "print('p-value: {}'.format(result[1]))\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# in order to mute the errors relating to inferred frequency \n",
    "order_aic_bic_boxcox=[]\n",
    "#loop over AR order\n",
    "for p in range(4):\n",
    "    #loop over MA order:\n",
    "    for q in range(4):\n",
    "        #fit the model\n",
    "        model_boxcox = SARIMAX(df_boxcox, order=(p,1,q), trend='c')\n",
    "        results_boxcox=model_boxcox.fit()\n",
    "        # print the model order and the AIC/BIC scores\n",
    "        order_aic_bic_boxcox.append((p, q, results_boxcox.aic, results_boxcox.bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_boxcox = pd.DataFrame(order_aic_bic_boxcox, columns = ['p', 'q', 'aic', 'bic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_aic_boxcox = order_df_boxcox.sort_values('aic')\n",
    "order_aic_sorted_boxcox = order_aic_boxcox.reindex(order_aic_boxcox.aic.abs().sort_values(ascending=False).index)\n",
    "order_aic_sorted_best_boxcox = order_aic_sorted_boxcox.iloc[0]\n",
    "print(order_aic_sorted_boxcox.head(3))\n",
    "print(order_aic_sorted_best_boxcox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model 012 has the lowest AIC score and thus has the best predictive power of the ARIMA models run. Therefore an ARIMA(0,1,2) model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_boxcox =int(order_aic_sorted_best_boxcox.values[0]) #assigning optimal values for p and q for the following ARIMA modelling. \n",
    "d_boxcox =1\n",
    "q_boxcox =int(order_aic_sorted_best_boxcox.values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code sets parameter values for P and Q based off of the sorting function to find the lowest AIC values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model_boxcox=SARIMAX(df_boxcox, order=(p_boxcox,d_boxcox,q_boxcox), trend='c')\n",
    "results_boxcox = model_boxcox.fit()\n",
    "#assign residuals to variable\n",
    "resid_boxcox = results_boxcox.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_boxcox.plot(kind='kde', title='Density Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean absolute errors of the residuals\n",
    "mae_boxcox = np.mean(np.abs(resid_boxcox))\n",
    "results_boxcox.plot_diagnostics(figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_boxcox.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Using the Boxcox transformation has made the results worse. Therefore, we will use the original dataset. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making forecasts\n",
    "## One-step-ahead forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasts will be made using an ARIMA(0,1,2) model, since the autoregressive order is zero this means that the model is only a moving average model, however ARIMA will be used since it forecasts using the first difference automatically and as such the the data is stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model_osa=SARIMAX(df, order=(p_lev,d_lev,q_lev), trend='c')\n",
    "results_osa = model_osa.fit()\n",
    "#assign residuals to variable\n",
    "resid_osa = results_osa.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_osa.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_osa = results_osa.get_prediction(start=-len_test, end=df.index[-1])\n",
    "#forecast mean\n",
    "mean_forecast_osa = forecast_osa.predicted_mean\n",
    "#get a confidence interval of forecasts\n",
    "confidence_intervals_osa = forecast_osa.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(mean_forecast_osa.index, mean_forecast_osa, color='r', label='Forecast')\n",
    "plt.plot(df_test, label='Testing dataset')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.title('One step ahead forecast using ARIMA - US \\$ into Sterling £')\n",
    "plt.fill_between(mean_forecast_osa.index, confidence_intervals_osa['lower GBPUSD'], confidence_intervals_osa['upper GBPUSD'], color='pink', alpha=0.7)\n",
    "\n",
    "legend_elements = [Line2D(df_test.index, df_test.values, color='b', label='Testing Dataset'),\n",
    "                   Line2D(mean_forecast_osa.index, mean_forecast_osa, color='r', label='Forecast'),\n",
    "                   Patch(facecolor='pink', label='5% confidence interval of the forecast')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "training_model=SARIMAX(df_train, order=(p_lev,d_lev,q_lev), trend='c', dynamic=True)\n",
    "training_results = training_model.fit()\n",
    "#assign residuals to variable\n",
    "training_resid = training_results.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_forecast = training_results.get_prediction(start = df_train.index[-1], end=df_test.index[-1])\n",
    "mean_insample_forecast = insample_forecast.predicted_mean\n",
    "insample_confidence_intervals = insample_forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(df_train)\n",
    "plt.plot(df_test)\n",
    "plt.plot(mean_insample_forecast.index, mean_insample_forecast, color='r', label='forecast')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.title('Dynamic forecast using ARIMA - US \\$ into Sterling £')\n",
    "plt.fill_between(mean_insample_forecast.index, insample_confidence_intervals['lower GBPUSD'], insample_confidence_intervals['upper GBPUSD'], color='pink', alpha=0.5)\n",
    "\n",
    "legend_elements_2 = [Line2D(df_train.index, df_train, color='blue', label='Training dataset'),\n",
    "                   Line2D(df_test.index, df_test, color='orange', label='Testing dataset'),\n",
    "                   Line2D(mean_insample_forecast.index, mean_insample_forecast, color='r', label='Dynamic forecast'),\n",
    "                   Patch(facecolor='pink', label='5% confidence interval of the forecast')]\n",
    "plt.legend(handles=legend_elements_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Sample Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oos = SARIMAX(df, order=(p_lev,d_lev,q_lev))\n",
    "result_oos = model_oos.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_forecast = result_oos.get_forecast(steps=364)\n",
    "oos_mean_forecast = oos_forecast.predicted_mean\n",
    "oos_confidence_intervals=oos_forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(oos_mean_forecast.index, oos_mean_forecast, color='r', label='forecast')\n",
    "plt.plot(df_test)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Spot exchange rate, US \\$ into Sterling £')\n",
    "plt.title('Out of sample forecast using ARIMA - US \\$ into Sterling £')\n",
    "plt.fill_between(oos_mean_forecast.index,\n",
    "                 oos_confidence_intervals['lower GBPUSD'],\n",
    "                 oos_confidence_intervals['upper GBPUSD'],\n",
    "                 color='pink', alpha=0.5)\n",
    "\n",
    "legend_elements_3 = [Line2D(df_test.index, df_test, color='blue', label='Testing dataset'),\n",
    "                   Line2D(mean_insample_forecast.index, mean_insample_forecast, color='r', label='Out of sample forecast'),\n",
    "                   Patch(facecolor='pink', label='5% confidence interval of the forecast')]\n",
    "plt.legend(handles=legend_elements_3, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All on one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_insample_forecast = pd.DataFrame(data=mean_insample_forecast, columns=['GBPUSD'])\n",
    "df_insample_confidence_intervals = pd.DataFrame(data=insample_confidence_intervals)\n",
    "df_mean_osa_forecast = pd.DataFrame(data=mean_forecast_osa, columns=['GBPUSD'])\n",
    "df_osa_confidence_intervals = pd.DataFrame(data=confidence_intervals_osa)\n",
    "df_mean_oos_forecast = pd.DataFrame(data=oos_mean_forecast, columns=['GBPUSD'])\n",
    "df_oos_confidence_intervals = pd.DataFrame(data=oos_confidence_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_1=ColumnDataSource(df_train)\n",
    "source_2=ColumnDataSource(df_mean_insample_forecast)\n",
    "source_3=ColumnDataSource(df_insample_confidence_intervals)\n",
    "source_4=ColumnDataSource(df_mean_osa_forecast)\n",
    "source_5=ColumnDataSource(df_osa_confidence_intervals)\n",
    "source_6=ColumnDataSource(df_test)\n",
    "source_7=ColumnDataSource(df_mean_oos_forecast)\n",
    "source_8=ColumnDataSource(df_oos_confidence_intervals)\n",
    "plot_2 = figure(x_axis_type='datetime', x_axis_label = 'Time', y_axis_label = 'Spot exchange rate, US $ into Sterling', title='Forecasting the GBP/USD exchange rate using ARIMA', tools=[hover, 'pan', 'wheel_zoom'], plot_width=1200)\n",
    "plot_2.line(x='index', y='GBPUSD', line_width=1.5, legend='Training Dataset', source=source_1, color='mediumaquamarine')\n",
    "plot_2.line(x='index', y='GBPUSD', line_width=1.5, legend='Testing Dataset', source=source_6, color='blue')\n",
    "plot_2.line(x='index', y='GBPUSD', line_width=1.5, legend='Step By Step Forecast', source=source_4, color='red')\n",
    "plot_2.line(x='index', y='GBPUSD', line_width=1.5, legend='Dynamic Forecast', source=source_2, color='green')\n",
    "plot_2.line(x='index', y='GBPUSD', line_width=1.5, legend='Out of sample Forecast', source=source_7, color='red')\n",
    "band = Band(base='index', lower='lower GBPUSD', upper='upper GBPUSD', source=source_5, level='underlay', fill_alpha=0.40, fill_color='purple')\n",
    "band_2 = Band(base='index', lower='lower GBPUSD', upper='upper GBPUSD', source=source_3, level='underlay', fill_alpha=0.30, fill_color='orange')\n",
    "band_3 = Band(base='index', lower='lower GBPUSD', upper='upper GBPUSD', source=source_8, level='underlay', fill_alpha=0.30, fill_color='grey')\n",
    "plot_2.legend.location = 'bottom_left'\n",
    "plot_2.legend.background_fill_color = 'lightgrey'\n",
    "plot_2.add_layout(band)\n",
    "plot_2.add_layout(band_2)\n",
    "plot_2.add_layout(band_3)\n",
    "output_file = ('forecast.html')\n",
    "show(plot_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion\n",
    "\n",
    "What did you find in the analysis above; what is the answer to the question you started out with.\n",
    "\n",
    "What are weaknesses of your approach that can be improved upon in future research (e.g. in your thesis).\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52e1cf5783dd1971d265a8ea172e297a",
     "grade": true,
     "grade_id": "cell-4b5c9f698b295ad7",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### Results\n",
    "The results from the ARIMA analysis suggests that in the future, the GBP/USD exchange rate will remain constant at a value of 1.307. The result implies that the out of sample forecast is unlikely to be accurate, since historically, as can be seen from the graphs, this exchange rate has varied greatly over the time period. In this respect, it could be assumed that ARIMA is not effective at forecasting the exchange rates. This flat line prediction could suggest that the model thinks that the data is a random walk, and as such the best prediction for the out of sample forecast is the last value in the time series. <br><br> On the other hand, the step by step forecast yields promising results. Using a training and testing dataset, the forecasted values are very close to the ex-post values of the testing dataset. Therefore, this single test of a single time period suggests that for one-step-ahead forecasts (in this case only one day ahead) ARIMA is reasonably accurate. However, in order to gain more insight we would have to compare this to different forecasting methodology, some potential options of which will be covered later in this section. It would, however, be concerning if the model could not closely follow the ex-post data in the step by step forecast because the forecast are calculated daily for only one day in advance.<br><br> The accuracy of the dynamic forecast made by the ARIMA model, fall somewhere in-between those made by the one-step-ahead forecasts and the out of sample forecasts. The prediction made by the model is that the exchange rate will depreciate, which was accurate in the period from April 2018 - August 2019. However, there were large fluctuations in the period of the dynamic forecast, and the ARIMA model has not taken these into account.If only the first and last periods of the forecast were viewed in isolation, it may seem that a linear, slightly upward trending forecast would be appropreate. The forecast period started with an exchange rate of 1.298 and ended at 1.307, however as the graph shows, this does not tell the full story of the time series. The maximum exchange rate reached in this period was 1.433 and the minimum was 1.206, a fluctuations not incorporated into the timeseries forecast. \n",
    "\n",
    "### Potential explanation\n",
    "There are many reasons why the ARIMA forecast could have been innacurate on all models, apart from the one-step-ahead model. ARIMA forecasts only take into account two factors, an auto-regressive element and a moving average element. The auto-regressive part of the ARIMA function implies that the value of the exchange rate depends on the past values of the exchange rate, and the moving average element assumes that the current value for the exchange rate depends on past forecast errors. <br><br> Exchange rates are very complicated instruments, and are subject to non-fundemental shocks, as outlined in the paper Straub & Tchakarov (2004), but also referred to in the papers: Jeanne & Rose (2002) and Gradojevic & Yang (2006). Non-fundamental exchange rate volatility is due to investors' expectations and not by changes in the economic fundamentals of the exchange rate.<br><br> In the case of the pound sterling, the Brexit referendum was likely to have a large effect on expectations surrounding the currency. This may be to blame for the large depreciation in the currency (seen between late 2015 and 2017) and the slump on and shortly after the 26$^{th}$ of June 2016 (Brexit referendum day). An ARIMA model forecasting this period would not have incorperated this large exogenous shock into the forecast.<br><br> This is a major issue (and weakness) when using ARIMA for such expectation and news orientated instruemnts. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Improvements\n",
    "A potential improvement could be to use an ARIMAX model, the same as an ARIMA model but including exogenous variables. In the case of exchange rates, this could be an expectation variable, however in practice this is unlikely to be easy to add to the model. The paper Osborn (1990) finds that \"only interest rates and the exchange rate exhibit no significant (deterministic or stochastic) seasonality\". Therefore, it is inappropriate to use SARIMA or SARIMAX methodology (which incorporates seasonality into the forecasts) for the exchange rate time series.<br><br> A more realistic improvement to the forecasting methodology would be to use Neural Networks, which has been done before in papers such as Yao & Tan (2000) with promising results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akaike, H. (1974). A new look at the statistical model identification. *IEEE transactions on automatic control* , 19(6), 716-723.<br><br>\n",
    "Box, G., & Cox, D. (1964). An Analysis of Transformations. *Journal of the Royal Statistical Society*. Series B (Methodological), 26(2), 211-252 <br><br>\n",
    "Box, G. E., & Jenkins, G. M. (1976). *Time series analysis. Forecasting and control.* In Holden-Day Series in Time Series Analysis, Revised ed., San Francisco: Holden-Day, 1976. <br><br>\n",
    "Burnham, K. P., & Anderson, D. R. (2002). *A practical information-theoretic approach. Model selection and multimodel inference* , 2nd ed. Springer, New York.<br><br>\n",
    "Gradojevic, N., & Yang, J. (2006). Non‐linear, non‐parametric, non‐fundamental exchange rate forecasting. *Journal of Forecasting* , 25(4), 227-245.<br><br>\n",
    "Hamilton, J. D. (1994). *Time series analysis*. Princeton, N.J: Princeton University Press <br><br>\n",
    "Jeanne, O., & Rose, A. K. (2002). Noise trading and exchange rate regimes. *The Quarterly Journal of Economics* , 117(2), 537-569.<br><br>\n",
    "Osborn, D. R. (1990). A survey of seasonality in UK macroeconomic variables. *International Journal of Forecasting* , 6(3), 327-336.<br><br>\n",
    "Pai, P. F., & Lin, C. S. (2005). A hybrid ARIMA and support vector machines model in stock price forecasting. *Omega* , 33(6), 497-505.<br><br>\n",
    "Parker, G. & Pickard, J. (2020, January 20). Johnson sets sights on speedy US and EU trade deals. *Financial Times* . Retrieved from https://www.ft.com/content/e5bd670e-3bab-11ea-a01a-bae547046735<br><br>\n",
    "Phillips, P. (1987). Time Series Regression with a Unit Root. *Econometrica* , 55(2), 277-301. <br><br>\n",
    "Office for National Statistics. (2018). *Who does the UK trade with?* Retrieved from https://www.ons.gov.uk/businessindustryandtrade/internationaltrade/articles/whodoestheuktradewith/2017-02-21<br><br>\n",
    "Straub, R., & Tchakarov, I. (2004). *Non-fundamental exchange rate volatility and welfare*_ Working Paper Series 328, European Central Bank. <br><br>\n",
    "Yao, J., & Tan, C. L. (2000). A case study on using neural networks to perform technical forecasting of forex. *Neurocomputing* , 34(1-4), 79-98.<br><br>\n",
    "Zhang, G. P. (2003). Time series forecasting using a hybrid ARIMA and neural network model. *Neurocomputing* , 50, 159-175.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
